id: 6971496daf3b3d8cb59d8535_documentation
summary: Lab 9: Agent Runtime Constraint Simulator Documentation
feedback link: https://docs.google.com/forms/d/e/1FAIpQLSfWkOK-in_bMMoHSZfcIvAeO58PAH9wrDqcxnJABHaxiDqhSA/viewform?usp=sf_link
environments: Web
status: Published
# QuLab: Lab 9: Agent Runtime Constraint Simulator

## 1. Understanding Agent Runtime Governance with QuLab
Duration: 05:00

<aside class="positive">
This step provides critical context for the codelab, explaining the "why" behind the application and its importance in modern AI governance.
</aside>

Welcome, fellow Platform Engineer! My name is Alex, and I work at QuantAlgo Solutions, a cutting-edge fintech firm. My primary responsibility is to ensure that our innovative AI agents operate within strict corporate governance, security, and financial controls. We're on the verge of deploying a new 'Market Data Analyst Agent' that will interact with various internal systems, but before it goes live, I need to thoroughly validate its runtime policies and guardrails. This involves setting up a secure, simulated environment, defining its operational boundaries, and then verifying that the agent adheres to these rules under different scenarios.

This application, **QuLab: Lab 9: Agent Runtime Constraint Simulator**, will walk us through the critical pre-deployment validation steps. We'll define the agent's available tools, configure its operational policies (like budget limits and approval gates), simulate its tasks, and meticulously audit its behavior. Our goal is to generate concrete evidence that the agent is safe, compliant, and ready for production.

### Purpose & Positioning
This lab operationalizes agentic AI risk control by simulating an autonomous agent operating under explicit runtime constraints, policies, and approvals.

It answers the enterprise question:
> Can this agent be trusted to act autonomously without violating safety, cost, or authorization boundariesâ€”and can we audit every step it takes?

This lab treats agents as stateful systems, not prompts with loops.

### Core Concepts Explained
Throughout this codelab, we will delve into the following crucial concepts:
*   **Tool Registry:** Defining and cataloging the capabilities and inherent risks of tools an agent can use.
*   **Agent Policy:** Establishing explicit rules, constraints, and guardrails for agent behavior.
*   **Task Definitions:** Crafting strategic test cases to rigorously validate agent policies.
*   **Agent State Machine:** Modeling the agent's execution flow as a deterministic sequence of states.
*   **Policy Engine:** The core mechanism for real-time validation of proposed agent actions against defined policies.
*   **Execution Trace & Violations Summary:** Generating audit-grade logs of agent actions and policy decisions.
*   **Artifact Export & Evidence Manifest:** Creating auditable reports and cryptographic evidence of policy compliance.

### High-Level Architecture
The application simulates an agent's interaction with tools, mediated by a robust policy engine.

```mermaid
graph TD
    A[Streamlit UI] --> B{Agent Simulator};
    B --> C[Agent Logic (Planning/Acting)];
    C --> D{Policy Engine};
    D -- Policy Decision (Approved/Denied/Approval Required) --> C;
    C -- Validated Action --> E[Mock Tools];
    E -- Tool Output --> C;
    D -- Fetches --> F[Agent Policy];
    D -- Fetches --> G[Tool Registry];
    B -- Defines --> H[Task Definitions];
    B -- Generates --> I[Execution Trace];
    B -- Generates --> J[Violations Summary];
    B -- Saves All As Artifacts --> K[Output Directory];
```
*   The **Streamlit UI** provides an interactive interface for configuring policies, tools, and tasks, and for triggering simulations.
*   The **Agent Simulator** orchestrates the execution of tasks, mimicking an agent's workflow.
*   The **Agent Logic** proposes actions based on the current task.
*   The **Policy Engine** is the critical gatekeeper, evaluating each proposed action against the **Agent Policy** and **Tool Registry**.
*   **Mock Tools** simulate real-world system interactions without actual side effects.
*   **Execution Trace** and **Violations Summary** are comprehensive logs generated by the simulator, providing full auditability.
*   All configuration and output files are saved as **Artifacts** for later review and auditing.

## 2. Setting up Your Development Environment
Duration: 02:00

To get started with this codelab, ensure you have Python 3.8+ and `pip` installed.

1.  **Clone the Repository (if not already done):**
    ```bash
    git clone https://github.com/your-repo/qu-lab-9-agent-simulator.git # Replace with actual repo URL
    cd qu-lab-9-agent-simulator
    ```

2.  **Create a Virtual Environment:**
    ```bash
    python -m venv venv
    ```

3.  **Activate the Virtual Environment:**
    *   On Windows:
        ```bash
        .\venv\Scripts\activate
        ```
    *   On macOS/Linux:
        ```bash
        source venv/bin/activate
        ```

4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    (Assuming `requirements.txt` contains `streamlit` and any other necessary libraries.)

5.  **Run the Streamlit Application:**
    ```bash
    streamlit run app.py
    ```
    This will open the application in your web browser, typically at `http://localhost:8501`.

6.  **Configure OpenAI API Key (Optional):**
    In the left sidebar, you'll find an input field for "OpenAI API Key (Optional)". While the core simulation logic provided uses mock functions and doesn't *require* an OpenAI API key for basic functionality, some advanced agent behaviors (e.g., using an LLM to generate actions) might depend on it. For this codelab, you can leave it blank unless explicitly instructed otherwise by the `source.py` implementation.

## 3. Initializing Sample Data
Duration: 01:00

Before we can define or simulate anything, we need to ensure our application has some baseline data for the agent's tools, policies, and tasks. The application provides a convenient way to do this.

1.  **Navigate to the Sidebar:** On the left side of the Streamlit application, locate the "Data Management" section.
2.  **Click "Initialize/Reset Sample Data":**
    <aside class="positive">
    Clicking this button will populate the session state with pre-defined sample data for the tool registry, agent policy, and task definitions. It also clears any previous simulation results, providing a fresh start.
    </aside>
    You will see a success message: "Sample data initialized!". The page will automatically navigate to the "Overview" section.

This action effectively loads the initial configurations from hypothetical JSON files (e.g., `tool_registry.json`, `agent_policy.json`, `task_definitions.json` as managed by `source.py`) into the Streamlit session state, making them available for editing and simulation.

## 4. Defining the Agent's Tool Registry
Duration: 05:00

The Market Data Analyst Agent at QuantAlgo Solutions will interact with several internal and external tools. As a Platform Engineer, I need to define each tool meticulously, detailing its purpose, access level, and associated risk. This registry will serve as the definitive list of tools our agent is allowed to *know about*. Each tool will also have a `mock_function` to simulate its actual behavior without making real API calls.

For instance, a 'MarketDataAPI_Read' tool would be `read-only` and `low` risk, while a 'Portfolio_Update' tool would be `write` and `critical` risk. This distinction is crucial for setting up our guardrails.

The tool registry adheres to an Authorization Matrix structure:
$$
\text{Authorization Matrix} = \begin{bmatrix}
T_1 & A(T_1) & R(T_1) \\
T_2 & A(T_2) & R(T_2) \\
\vdots & \vdots & \vdots \\
T_N & A(T_N) & R(T_N) \\
\end{bmatrix}
$$
where $T_i$ represents tool $i$, $A(T_i)$ is its access level (`read-only`, `write`, `execute`), and $R(T_i)$ is its risk class (`low`, `medium`, `high`, `critical`).

The purpose of defining this registry is to establish the base capabilities and inherent risks of each function the agent might invoke. This forms the first layer of our security model.

1.  **Navigate to "Tool Registry Editor":** In the sidebar, select "Tool Registry Editor" from the "Choose a Section" dropdown.
2.  **Review the Current Tool Registry:** You will see a data editor displaying the tools. Each tool has the following attributes:
    *   **Tool Name:** A unique identifier for the tool.
    *   **Description:** A brief explanation of what the tool does.
    *   **Access Level:** Defines the type of operation (e.g., `read-only`, `write`, `execute`).
    *   **Risk Class:** Categorizes the inherent risk of using the tool (e.g., `low`, `medium`, `high`, `critical`).
    *   **Mock Function Name:** The name of the Python function (from `source.py`) that simulates this tool's behavior.
3.  **Edit/Add Tools:**
    *   You can directly edit values in the table cells.
    *   Click the "Add row" button at the bottom of the table to add a new tool.
    *   Ensure each new tool has all required fields populated.
    *   Experiment with changing the `risk_class` of an existing tool, e.g., change `MarketDataAPI_Read` to `medium`.
4.  **Update Tool Registry:** After making changes, click the "Update Tool Registry" button. This saves your modifications to the application's session state.

```json
[
  {
    "tool_name": "MarketDataAPI_Read",
    "description": "Reads real-time and historical market data.",
    "access_level": "read-only",
    "risk_class": "low",
    "mock_function_name": "mock_read_market_data"
  },
  {
    "tool_name": "Portfolio_Analysis",
    "description": "Performs detailed analysis on portfolio performance.",
    "access_level": "read-only",
    "risk_class": "medium",
    "mock_function_name": "mock_analyze_portfolio"
  }
]
```
The above JSON snippet is an example of how tools are defined in the registry.

## 5. Crafting Agent Execution Policies
Duration: 07:00

Now that we know what tools our agent *can* potentially use, it's time to define the strict rules it *must* follow. As a Platform Engineer, I configure the `agent_policy.json` to enforce critical guardrails like allowed tools, maximum execution steps, budget limits (representing cost in tokens or compute), and explicit approval gates for sensitive operations.

This policy is the cornerstone of our agent's safe operation. Without it, an autonomous agent could easily spiral out of control, incurring excessive costs or performing unauthorized actions. For instance, we'll ensure the Market Data Analyst Agent cannot access the `System_Config_Change` tool, and any `Portfolio_Update` action requires explicit human approval due to its `critical` risk class.

The Policy Engine makes decisions based on a Policy Function:
$$
\text{Policy Function} P(\text{action, state}) \rightarrow \text{Decision} \in \{\text{Approved, Denied, Approval Required}\}
$$
where the decision is based on a set of logical conditions:
*   **Tool Permission Check:** Is $T_{\text{proposed}} \in T_{\text{allowed}}$?
*   **Step Limit Check:** Is $S_{\text{current}} < S_{\text{max}}$?
*   **Budget Limit Check:** Is $C_{\text{action}} + C_{\text{current}} \leq C_{\text{max}}$?
*   **Approval Gate Check:** Is $R(T_{\text{proposed}}) \geq R_{\text{threshold}}$ or $A(T_{\text{proposed}}) \in A_{\text{approval\_required}}$?

If any of these conditions are not met, a violation or approval requirement is triggered.

1.  **Navigate to "Policy Editor":** In the sidebar, select "Policy Editor".
2.  **Review and Configure Current Agent Policy:**
    *   **Allowed Tools:** Use the multiselect dropdown to specify which tools the agent is permitted to use. Only tools defined in the "Tool Registry" will appear here. Try removing `MarketDataAPI_Read` to simulate a forbidden action later.
    *   **Max Steps Per Run:** Set the maximum number of actions the agent can take in a single simulation run. This prevents infinite loops.
    *   **Budget Limit (e.g., tokens/cost proxy):** Define a numerical budget for the agent's actions. Each tool action consumes a "cost," and this limit prevents overspending.
    *   **Approval Requirements:**
        *   **Approval Required for Access Levels:** Select access levels (e.g., `write`, `execute`) that will *always* require human approval, regardless of risk class.
        *   **Approval Required for Risk Classes:** Select risk classes (e.g., `high`, `critical`) that will *always* require human approval.
    *   **Escalation Rule:** A text field to describe what happens when a severe policy violation occurs (e.g., "Notify Security Team and Terminate Agent").
3.  **Update Agent Policy:** After making your desired changes, click the "Update Agent Policy" button to save them to the session state.

<aside class="negative">
Careful policy configuration is crucial. Misconfigurations here can either lead to an overly restrictive agent unable to perform its tasks or an overly permissive agent that poses significant risks.
</aside>

## 6. Defining Simulation Tasks
Duration: 05:00

With our tools and policies in place, the next crucial step is to define specific tasks for our agent to perform in the simulation. These `task_definitions.json` are not just random assignments; they are carefully crafted test cases designed to validate our policies under different conditions. As a Platform Engineer, I need to ensure these tasks will trigger:
1.  A standard, compliant execution.
2.  A policy violation (e.g., attempting a disallowed tool or exceeding limits).
3.  A scenario requiring explicit human approval.

This strategic task definition is key to thoroughly stress-testing our guardrails.

1.  **Navigate to "Task Runner":** In the sidebar, select "Task Runner".
2.  **Review Current Task Definitions:** You will see a data editor for tasks. Each task has the following attributes:
    *   **Task ID:** A unique identifier for the task.
    *   **Task Description:** A natural language description of what the agent needs to achieve.
    *   **Expected Actions (JSON Array):** This is the core of our test case. It defines a sequence of *expected* tool calls, including `tool_name`, `params` (parameters for the mock function), and `cost`. These are the actions the simulator will attempt to execute, allowing us to pre-program scenarios that test specific policy conditions.
        ```json
        [
          {"tool_name": "MarketDataAPI_Read", "params": {"query": "tech stock trends"}, "cost": 10},
          {"tool_name": "Portfolio_Analysis", "params": {"portfolio_id": "QA-001"}, "cost": 15}
        ]
        ```
    *   **Expected Outcome:** A description of what is anticipated to happen when the agent attempts this task (e.g., "Successfully complete task," "Trigger budget violation," "Require approval").
3.  **Edit/Add Tasks:**
    *   You can directly edit values in the table cells.
    *   Add a new row to create a new task.
    *   **Challenge:** Create a task that you expect to trigger a budget violation by setting the `cost` of its `expected_actions` to exceed the `budget_limit` you set in the Policy Editor.
    *   **Challenge:** Create a task that uses a tool with an `access_level` or `risk_class` that you configured to require approval in the Policy Editor.
4.  **Update Task Definitions:** After making changes, click the "Update Task Definitions" button. This saves your modifications to the application's session state.

## 7. Executing the Agent Simulation
Duration: 03:00

With our tools, policies, and tasks meticulously defined, we are ready to run the simulation. This step invokes the `AgentSimulator` to execute each defined task under the watchful eye of the `PolicyEngine`.

1.  **Ensure Configurations are Complete:** Before proceeding, verify that:
    *   The "Tool Registry Editor" has the tools you intend to use.
    *   The "Policy Editor" has the desired guardrails configured.
    *   The "Task Runner" has tasks defined, especially those designed to test specific policy conditions (violations, approvals).
    *   If any of these are incomplete, the simulation might produce an error or unexpected results.

2.  **Run Agent Simulation:** In the "Task Runner" section, locate and click the "Run Agent Simulation" button (primary button).
    <aside class="positive">
    This button orchestrates the entire simulation process. It initializes the `AgentSimulator` with your current configurations, runs through all defined tasks, and collects the audit trail.
    </aside>
    *   A spinner will appear, indicating the simulation is running.
    *   Once complete, a success message will display, showing the generated `Run ID`.
    *   The application will automatically navigate to the "Simulation & Results" page, where you can view the outcomes.

What happens behind the scenes:
*   The `AgentSimulator` class is instantiated with the current `tool_registry`, `agent_policy`, and `task_definitions` from the session state.
*   It then iterates through each task, simulating the agent's actions.
*   For every proposed action, the `PolicyEngine` (an internal component of the simulator) performs real-time checks against the configured policy.
*   All decisions, actions, and policy outcomes are recorded in an `execution_trace`.
*   Any policy violations or explicit approval requirements are summarized in a `violations_summary`.
*   Finally, all simulation artifacts (including `execution_trace.json`, `violations_summary.json`, `session09_executive_summary.md`, etc.) are saved to a unique, time-stamped directory under `reports/session09/`.

## 8. Analyzing Simulation Results and Policy Enforcement
Duration: 10:00

This is the engineering core of our validation. As a Platform Engineer, I need to implement a robust `AgentSimulator` that models the agent's behavior as a deterministic state machine. Crucially, before *each* simulated action, a `PolicyEngine` must intercede to check for violations against our defined `agent_policy.json` and `tool_registry.json`.

The agent will transition through states like `INIT`, `PLAN`, `ACT`, `REVIEW`, `APPROVAL_REQUIRED`, `COMPLETE`, or `VIOLATION`. This state machine ensures every decision and its outcome is traceable.

The `AgentSimulator` uses a state transition function:
$$
\text{State Transition Function} \delta(S_t, A_t, P_{\text{outcome}})
$$
where $S_t$ is the current state, $A_t$ is the proposed action, and $P_{\text{outcome}}$ is the policy engine's decision.

For each step $t$:
1.  Agent proposes action $A_t$.
2.  Policy Engine evaluates $P(A_t, S_t) \rightarrow P_{\text{outcome}}$.
3.  New state $S_{\text{t+1}} = \delta(S_t, A_t, P_{\text{outcome}})$.

Example transitions:
*   If $P_{\text{outcome}} = \text{APPROVED}$, then $S_{\text{t+1}} = \text{ACT}$.
*   If $P_{\text{outcome}} = \text{REQUIRES\_APPROVAL}$, then $S_{\text{t+1}} = \text{APPROVAL\_REQUIRED}$.
*   If $P_{\text{outcome}} = \text{DENIED\_VIOLATION}$, then $S_{\text{t+1}} = \text{VIOLATION}$.

The `PolicyEngine` also tracks resource consumption (steps, budget). For budget, if $C_{\text{action}}$ is the cost of the proposed action and $B_{\text{current}}$ is the remaining budget, the new budget $B_{\text{next}} = B_{\text{current}} - C_{\text{action}}$. A violation occurs if $B_{\text{next}} < 0$. Similarly for steps, if $S_{\text{current}}$ is the current step count and $S_{\text{max}}$ is the maximum, a violation occurs if $S_{\text{current}} + 1 > S_{\text{max}}$.

### Policy Engine Decision Flow
```mermaid
graph TD
    A[Agent Proposes Action (Tool, Params, Cost)] --> B{Policy Engine: Start};
    B --> C{Is Tool Allowed?};
    C -- No --> V1[Violation: Tool Not Allowed];
    C -- Yes --> D{Max Steps Exceeded?};
    D -- Yes --> V2[Violation: Max Steps Reached];
    D -- No --> E{Budget Exceeded?};
    E -- Yes --> V3[Violation: Budget Exceeded];
    E -- No --> F{Approval Required for Access Level / Risk Class?};
    F -- Yes --> R[Decision: Approval Required];
    F -- No --> G[Decision: Approved];
    V1 --> H[Log Violation & Terminate Task];
    V2 --> H;
    V3 --> H;
    R --> I[Log Approval Requirement & Pause Task];
    G --> J[Log Approved Action & Execute Tool];
```

### Reviewing Simulation Results
The simulation generated a wealth of data about the agent's behavior. My job as a Platform Engineer doesn't end with running the simulation; I must analyze the results, particularly the `violations_summary.json` and `execution_trace.json`, to confirm that policies were correctly enforced.

This step involves reviewing the audit logs to confirm that:
1.  Compliant actions proceeded without hindrance.
2.  Disallowed tool usage was correctly identified and stopped.
3.  Budget and step limits were enforced.
4.  Sensitive operations correctly triggered an `APPROVAL_REQUIRED` state.

Finally, I will generate a comprehensive `session09_executive_summary.md` report and an `evidence_manifest.json` (with SHA-256 hashes for integrity), providing concrete proof to stakeholders that the agent is ready for deployment. This output is our deliverable, ensuring auditability and confidence in the agent's safety.

1.  **View Execution Trace:** This table provides a detailed, step-by-step log of the agent's simulation run. Each row represents an action taken (or attempted) by the agent, including:
    *   `run_id`: The identifier for the simulation run.
    *   `task_id`: The specific task being executed.
    *   `step`: The sequence number of the action within the task.
    *   `tool_name`: The tool the agent attempted to use.
    *   `params`: The parameters passed to the tool.
    *   `cost`: The cost associated with this action.
    *   `policy_decision`: The outcome from the Policy Engine (e.g., `APPROVED`, `DENIED_TOOL_NOT_ALLOWED`, `APPROVAL_REQUIRED_RISK_CLASS`).
    *   `state`: The agent's state after the action (e.g., `ACT`, `VIOLATION`, `APPROVAL_REQUIRED`).
    *   `remaining_budget`: The budget left after this action.
    *   `output`: The simulated output from the tool, or an error/policy message.

    Scroll through the `execution_trace` to observe how different tasks unfolded. Look for `policy_decision` and `state` columns to understand why certain actions were permitted, denied, or flagged for approval.

2.  **View Violation Summary:** This table provides a concise summary of all policy violations and approval requirements encountered during the simulation. It includes:
    *   `run_id`, `task_id`, `step`, `tool_name`: Context for the incident.
    *   `violation_type`: The specific policy breach (e.g., `TOOL_NOT_ALLOWED`, `BUDGET_EXCEEDED`, `APPROVAL_REQUIRED_RISK_CLASS`).
    *   `details`: A more descriptive message about the violation.

    This summary is particularly useful for quickly identifying areas where the agent attempted to deviate from its defined policies. If no violations appear, it suggests either a perfectly compliant agent or insufficient test cases.

    <aside class="positive">
    Analyzing the `execution_trace` and `violations_summary` is crucial for validating your policy configurations. If an expected violation didn't occur, or an unexpected one did, it indicates an issue either with your policy definition or your task design.
    </aside>

## 9. Reviewing and Exporting Simulation Artifacts
Duration: 04:00

All generated output artifacts are stored in a run-specific directory within `reports/session09/`.

The following artifacts are produced for each simulation run:
*   `tool_registry.json`: A snapshot of the tool registry configuration used for the run.
*   `agent_policy.json`: A snapshot of the agent policy configuration used for the run.
*   `task_definitions.json`: A snapshot of the task definitions used for the run.
*   `execution_trace.json`: The detailed, step-by-step log of the agent's actions and policy decisions.
*   `violations_summary.json`: A summary of all policy violations and approval requirements.
*   `session09_executive_summary.md`: A human-readable report summarizing the simulation's findings.
*   `config_snapshot.json`: A consolidated view of all configurations at the time of the run.
*   `evidence_manifest.json`: A manifest listing all generated artifacts along with their SHA-256 hashes, ensuring data integrity and auditability.

All artifacts within the evidence manifest are hashed with SHA-256 to ensure data integrity and auditability.

1.  **Navigate to "Export Panel":** In the sidebar, select "Export Panel".
2.  **View Last Simulation Output:** The page will display the `Run ID` of the last simulation and the directory where artifacts are stored.
3.  **Download All Artifacts:** You will see a "Download All Artifacts as .zip" link and a "Download Artifacts Zip" button. Click either to download a zip archive containing all the generated files for that specific simulation run. This is your comprehensive audit package.

    <button>
      [Download Artifacts Zip](data:application/zip;base64,BASE64_ENCODED_ZIP_DATA)
    </button>
    (Note: The actual `data:` URL will be generated dynamically by Streamlit when the application runs.)

4.  **Review Executive Summary:** The content of the `session09_executive_summary.md` file will be displayed directly within the application, providing a high-level overview of the simulation results, including:
    *   Simulation details.
    *   Policy highlights.
    *   Summary of compliant actions.
    *   Summary of policy violations (if any).
    *   Summary of approval requirements (if any).
    *   Conclusion and recommendations.

    This markdown report is designed to be shared with stakeholders who need a quick, yet comprehensive, understanding of the agent's compliance posture.

5.  **Review Evidence Manifest:** The content of the `evidence_manifest.json` file will also be displayed. This JSON document lists each artifact produced by the simulation and its corresponding SHA-256 hash. This cryptographic evidence is crucial for audit trails, ensuring that the generated reports and logs have not been tampered with since their creation.

    ```json
    {
      "run_id": "...",
      "timestamp": "...",
      "artifacts": [
        {
          "filename": "tool_registry.json",
          "path": "reports/session09/...",
          "sha256": "abcdef12345..."
        },
        {
          "filename": "execution_trace.json",
          "path": "reports/session09/...",
          "sha256": "fedcba98765..."
        }
        // ... more artifacts
      ]
    }
    ```
    This manifest serves as verifiable proof of the integrity of your simulation results.

Congratulations! You have successfully navigated the **QuLab: Lab 9: Agent Runtime Constraint Simulator**, from defining agent capabilities and policies to simulating its behavior under strict guardrails and auditing its every step. You now have the tools and understanding to validate and ensure the trustworthiness of autonomous AI agents in enterprise environments.
